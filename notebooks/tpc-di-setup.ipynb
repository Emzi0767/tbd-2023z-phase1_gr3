{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90a0ed7a-a23a-4523-9171-f1d076700168",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_BUCKET=tbd-2023z-1000-data\n",
      "env: GEN_OUTPUT_DIR=/tmp/tpc-di\n",
      "env: REPO_ROOT=/home/jupyter/tbd-tpc-di_gr3\n"
     ]
    }
   ],
   "source": [
    "%env DATA_BUCKET=tbd-2023z-1000-data\n",
    "%env GEN_OUTPUT_DIR=/tmp/tpc-di\n",
    "%env REPO_ROOT=/home/jupyter/tbd-tpc-di_gr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0fbec0d-d410-4de9-8dfd-00b07fdb7b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typer==0.9.0 in /usr/local/lib/python3.8/dist-packages (from typer[All]==0.9.0) (0.9.0)\n",
      "Requirement already satisfied: google-cloud-storage==2.13.0 in /usr/local/lib/python3.8/dist-packages (2.13.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.26.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.15.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (1.5.0)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from typer[All]==0.9.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from typer[All]==0.9.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.8/dist-packages (from typer[All]==0.9.0) (13.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0) (1.62.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0) (4.25.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2019.11.28)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[All]==0.9.0) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0) (0.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3.8 install typer[All]==0.9.0 google-cloud-storage==2.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b5820-852d-4c30-a178-9dd78bbdbd5f",
   "metadata": {},
   "source": [
    "## Install SDKMAN for setting up JVM 8 enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a3cdb5b-4978-479e-85c7-c9dbac3e65fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                -+syyyyyyys:\n",
      "                            `/yho:`       -yd.\n",
      "                         `/yh/`             +m.\n",
      "                       .oho.                 hy                          .`\n",
      "                     .sh/`                   :N`                `-/o`  `+dyyo:.\n",
      "                   .yh:`                     `M-          `-/osysoym  :hs` `-+sys:      hhyssssssssy+\n",
      "                 .sh:`                       `N:          ms/-``  yy.yh-      -hy.    `.N-````````+N.\n",
      "               `od/`                         `N-       -/oM-      ddd+`     `sd:     hNNm        -N:\n",
      "              :do`                           .M.       dMMM-     `ms.      /d+`     `NMMs       `do\n",
      "            .yy-                             :N`    ```mMMM.      -      -hy.       /MMM:       yh\n",
      "          `+d+`           `:/oo/`       `-/osyh/ossssssdNMM`           .sh:         yMMN`      /m.\n",
      "         -dh-           :ymNMMMMy  `-/shmNm-`:N/-.``   `.sN            /N-         `NMMy      .m/\n",
      "       `oNs`          -hysosmMMMMydmNmds+-.:ohm           :             sd`        :MMM/      yy\n",
      "      .hN+           /d:    -MMMmhs/-.`   .MMMh   .ss+-                 `yy`       sMMN`     :N.\n",
      "     :mN/           `N/     `o/-`         :MMMo   +MMMN-         .`      `ds       mMMh      do\n",
      "    /NN/            `N+....--:/+oooosooo+:sMMM:   hMMMM:        `my       .m+     -MMM+     :N.\n",
      "   /NMo              -+ooooo+/:-....`...:+hNMN.  `NMMMd`        .MM/       -m:    oMMN.     hs\n",
      "  -NMd`                                    :mm   -MMMm- .s/     -MMm.       /m-   mMMd     -N.\n",
      " `mMM/                                      .-   /MMh. -dMo     -MMMy        od. .MMMs..---yh\n",
      " +MMM.                                           sNo`.sNMM+     :MMMM/        sh`+MMMNmNm+++-\n",
      " mMMM-                                           /--ohmMMM+     :MMMMm.       `hyymmmdddo\n",
      " MMMMh.                  ````                  `-+yy/`yMMM/     :MMMMMy       -sm:.``..-:-.`\n",
      " dMMMMmo-.``````..-:/osyhddddho.           `+shdh+.   hMMM:     :MmMMMM/   ./yy/` `:sys+/+sh/\n",
      " .dMMMMMMmdddddmmNMMMNNNNNMMMMMs           sNdo-      dMMM-  `-/yd/MMMMm-:sy+.   :hs-      /N`\n",
      "  `/ymNNNNNNNmmdys+/::----/dMMm:          +m-         mMMM+ohmo/.` sMMMMdo-    .om:       `sh\n",
      "     `.-----+/.`       `.-+hh/`         `od.          NMMNmds/     `mmy:`     +mMy      `:yy.\n",
      "           /moyso+//+ossso:.           .yy`          `dy+:`         ..       :MMMN+---/oys:\n",
      "         /+m:  `.-:::-`               /d+                                    +MMMMMMMNh:`\n",
      "        +MN/                        -yh.                                     `+hddhy+.\n",
      "       /MM+                       .sh:\n",
      "      :NMo                      -sh/\n",
      "     -NMs                    `/yy:\n",
      "    .NMy                  `:sh+.\n",
      "   `mMm`               ./yds-\n",
      "  `dMMMmyo:-.````.-:oymNy:`\n",
      "  +NMMMMMMMMMMMMMMMMms:`\n",
      "    -+shmNMMMNmdy+:`\n",
      "\n",
      "\n",
      "                                                                 Now attempting installation...\n",
      "\n",
      "\n",
      "Looking for a previous installation of SDKMAN...\n",
      "SDKMAN found.\n",
      "\n",
      "======================================================================================================\n",
      " You already have SDKMAN installed.\n",
      " SDKMAN was found at:\n",
      "\n",
      "    /root/.sdkman\n",
      "\n",
      " Please consider running the following if you need to upgrade.\n",
      "\n",
      "    $ sdk selfupdate force\n",
      "\n",
      "======================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s https://get.sdkman.io | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691084f8-a883-408a-8d16-cc717669c04d",
   "metadata": {},
   "source": [
    "## Install and set as default JVM 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "275488e6-68e3-4f42-a3ca-060b286bade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;33mjava 8.0.392-amzn is already installed.\u001b[0m\n",
      "\n",
      "\u001b[1;32mUsing java version 8.0.392-amzn in this shell.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 8.0.392-amzn\n",
    "sdk use java 8.0.392-amzn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554db58-b832-45e2-a1f6-28bd873ae31d",
   "metadata": {},
   "source": [
    "## Check if JVM 8 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "898835b7-c347-49dd-bc0f-ca845375e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_392\"\n",
      "OpenJDK Runtime Environment Corretto-8.392.08.1 (build 1.8.0_392-b08)\n",
      "OpenJDK 64-Bit Server VM Corretto-8.392.08.1 (build 25.392-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45d9a3-e599-4661-944b-1bdfe3808c19",
   "metadata": {},
   "source": [
    "## Clone tbd-tpc-di repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63b4bc80-2bee-441c-b95a-fcabacfb86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -r git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7481d445-4ea6-40cc-8320-5520fe8d4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 2: cd: git: No such file or directory\n",
      "Cloning into 'tbd-tpc-di_gr3'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# mkdir -p git && \n",
    "cd git\n",
    "git clone https://github.com/karpinskiJ/tbd-tpc-di_gr3.git\n",
    "cd tbd-tpc-di_gr3\n",
    "git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b992bfc-8f8c-42eb-bf07-069b0d897fda",
   "metadata": {},
   "source": [
    "## Generate input dataset (run this cell below from the terminal!!!)\n",
    "It should take approx. 15min with scale factor set to 100 and generate approx. 10GiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a5331-415f-4ef2-b93a-4910b21ef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd /home/jupyter/git/tbd-tpc-di_gr3/tools/ \n",
    "java -jar DIGen.jar -sf 10 -o /tmp/tpc-di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24c0aa-bff7-4831-869d-3183e7373a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup JVM 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62e466e8-41aa-4afc-8392-5675d36adcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: java 11.0.21-amzn\n",
      "\n",
      "In progress...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repackaging Java 11.0.21-amzn...\n",
      "\n",
      "Done repackaging...\n",
      "\n",
      "\u001b[1;32mInstalling: java 11.0.21-amzn\u001b[0m\n",
      "\u001b[1;32mDone installing!\u001b[0m\n",
      "\n",
      "\u001b[1;33mDo you want java 11.0.21-amzn to be set as default? (Y/n): \u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 11.0.21-amzn\n",
    "sdk use java 11.0.21-amzn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccda4c-40d4-40fe-9d2e-68b416522f64",
   "metadata": {},
   "source": [
    "## Load staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "259b72bc-2efd-405b-883f-618a9772bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1b5c0d68-0787-4450-9915-2de3f9285ad0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.17.0/spark-xml_2.12-0.17.0.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#spark-xml_2.12;0.17.0!spark-xml_2.12.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.11.0!commons-io.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/jaxb/txw2/3.0.2/txw2-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish.jaxb#txw2;3.0.2!txw2.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/ws/xmlschema/xmlschema-core/2.3.0/xmlschema-core-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.ws.xmlschema#xmlschema-core;2.3.0!xmlschema-core.jar(bundle) (47ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.9.0/scala-collection-compat_2.12-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.9.0!scala-collection-compat_2.12.jar (87ms)\n",
      ":: resolution report :: resolve 9551ms :: artifacts dl 362ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1b5c0d68-0787-4450-9915-2de3f9285ad0\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (989kB/93ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 13:44:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 13:44:10 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/07 13:44:10 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/07 13:44:20 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/07 13:44:20 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/07 13:44:20 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/07 13:44:20 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/07 13:44:20 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/01/07 13:44:46 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "24/01/07 13:46:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "DATE table created.\n",
      "DAILY_MARKET table created.\n",
      "INDUSTRY table created.\n",
      "PROSPECT table created.\n",
      "CUSTOMER_MGMT table created.\n",
      "TAX_RATE table created.\n",
      "HR table created.\n",
      "WATCH_HISTORY table created.\n",
      "TRADE table created.\n",
      "TRADE_HISTORY table created.\n",
      "STATUS_TYPE table created.\n",
      "TRADE_TYPE table created.\n",
      "HOLDING_HISTORY table created.\n",
      "CASH_TRANSACTION table created.\n",
      "CMP table created.\n",
      "SEC table created.\n",
      "FIN table created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd $REPO_ROOT\n",
    "python3.8 tpcdi.py --output-directory $GEN_OUTPUT_DIR --stage $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02842209-e2d0-414e-8afe-7923b691487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m13:51:13  Running with dbt=1.7.3\n",
      "\u001b[0m13:51:13  Updating lock file in file path: /home/jupyter/tbd-tpc-di_gr3/package-lock.yml\n",
      "\u001b[0m13:51:13  Installing dbt-labs/dbt_utils\n",
      "\u001b[0m13:51:13  Installed from version 1.1.1\n",
      "\u001b[0m13:51:13  Up to date!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0740193-5da4-4aac-9349-aa4e76f4e99b",
   "metadata": {},
   "source": [
    "## Run dbt ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98d0642c-b5c9-4aa8-996f-40f82e3c90e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m14:47:01  Running with dbt=1.7.3\n",
      "\u001b[0m14:47:02  Registered adapter: spark=1.7.1\n",
      "\u001b[0m14:47:03  Unable to do partial parsing because profile has changed\n",
      "\u001b[0m14:47:09  Found 44 models, 1 test, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m14:47:09  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dc226b4a-11b8-4c73-89bb-0538f47a68fd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 649ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dc226b4a-11b8-4c73-89bb-0538f47a68fd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/35ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 14:47:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 14:47:19 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/07 14:47:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/07 14:47:29 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:47:29 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:47:29 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/07 14:47:29 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:47:29 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:47:52 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m14:47:55  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m14:47:55  \n",
      "\u001b[0m14:47:55  1 of 43 START sql table model demo_bronze.brokerage_cash_transaction ........... [RUN]\n",
      "24/01/07 14:47:56 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/01/07 14:47:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m14:48:11  1 of 43 OK created sql table model demo_bronze.brokerage_cash_transaction ...... [\u001b[32mOK\u001b[0m in 16.01s]\n",
      "\u001b[0m14:48:11  2 of 43 START sql table model demo_bronze.brokerage_daily_market ............... [RUN]\n",
      "24/01/07 14:48:11 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:48:24  2 of 43 OK created sql table model demo_bronze.brokerage_daily_market .......... [\u001b[32mOK\u001b[0m in 13.53s]\n",
      "\u001b[0m14:48:24  3 of 43 START sql table model demo_bronze.brokerage_holding_history ............ [RUN]\n",
      "24/01/07 14:48:24 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:48:27  3 of 43 OK created sql table model demo_bronze.brokerage_holding_history ....... [\u001b[32mOK\u001b[0m in 2.55s]\n",
      "\u001b[0m14:48:27  4 of 43 START sql table model demo_bronze.brokerage_trade ...................... [RUN]\n",
      "24/01/07 14:48:27 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:48:34  4 of 43 OK created sql table model demo_bronze.brokerage_trade ................. [\u001b[32mOK\u001b[0m in 7.42s]\n",
      "\u001b[0m14:48:34  5 of 43 START sql table model demo_bronze.brokerage_trade_history .............. [RUN]\n",
      "24/01/07 14:48:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:48:40  5 of 43 OK created sql table model demo_bronze.brokerage_trade_history ......... [\u001b[32mOK\u001b[0m in 5.69s]\n",
      "\u001b[0m14:48:40  6 of 43 START sql table model demo_bronze.brokerage_watch_history .............. [RUN]\n",
      "24/01/07 14:48:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:48:46  6 of 43 OK created sql table model demo_bronze.brokerage_watch_history ......... [\u001b[32mOK\u001b[0m in 6.03s]\n",
      "\u001b[0m14:48:46  7 of 43 START sql table model demo_bronze.crm_customer_mgmt .................... [RUN]\n",
      "24/01/07 14:48:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/01/07 14:48:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "\u001b[0m14:48:49  7 of 43 OK created sql table model demo_bronze.crm_customer_mgmt ............... [\u001b[32mOK\u001b[0m in 2.93s]\n",
      "\u001b[0m14:48:49  8 of 43 START sql table model demo_bronze.finwire_company ...................... [RUN]\n",
      "24/01/07 14:48:49 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:48:50  8 of 43 OK created sql table model demo_bronze.finwire_company ................. [\u001b[32mOK\u001b[0m in 1.37s]\n",
      "\u001b[0m14:48:50  9 of 43 START sql table model demo_bronze.finwire_financial .................... [RUN]\n",
      "24/01/07 14:48:51 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:48:59  9 of 43 OK created sql table model demo_bronze.finwire_financial ............... [\u001b[32mOK\u001b[0m in 8.52s]\n",
      "\u001b[0m14:48:59  10 of 43 START sql table model demo_bronze.finwire_security .................... [RUN]\n",
      "24/01/07 14:48:59 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:00  10 of 43 OK created sql table model demo_bronze.finwire_security ............... [\u001b[32mOK\u001b[0m in 1.65s]\n",
      "\u001b[0m14:49:00  11 of 43 START sql table model demo_bronze.hr_employee ......................... [RUN]\n",
      "24/01/07 14:49:01 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:02  11 of 43 OK created sql table model demo_bronze.hr_employee .................... [\u001b[32mOK\u001b[0m in 1.73s]\n",
      "\u001b[0m14:49:02  12 of 43 START sql table model demo_bronze.reference_date ...................... [RUN]\n",
      "24/01/07 14:49:02 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:04  12 of 43 OK created sql table model demo_bronze.reference_date ................. [\u001b[32mOK\u001b[0m in 1.75s]\n",
      "\u001b[0m14:49:04  13 of 43 START sql table model demo_bronze.reference_industry .................. [RUN]\n",
      "24/01/07 14:49:04 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:05  13 of 43 OK created sql table model demo_bronze.reference_industry ............. [\u001b[32mOK\u001b[0m in 1.28s]\n",
      "\u001b[0m14:49:05  14 of 43 START sql table model demo_bronze.reference_status_type ............... [RUN]\n",
      "24/01/07 14:49:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:06  14 of 43 OK created sql table model demo_bronze.reference_status_type .......... [\u001b[32mOK\u001b[0m in 0.89s]\n",
      "\u001b[0m14:49:06  15 of 43 START sql table model demo_bronze.reference_tax_rate .................. [RUN]\n",
      "24/01/07 14:49:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:07  15 of 43 OK created sql table model demo_bronze.reference_tax_rate ............. [\u001b[32mOK\u001b[0m in 0.91s]\n",
      "\u001b[0m14:49:07  16 of 43 START sql table model demo_bronze.reference_trade_type ................ [RUN]\n",
      "24/01/07 14:49:07 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:08  16 of 43 OK created sql table model demo_bronze.reference_trade_type ........... [\u001b[32mOK\u001b[0m in 0.85s]\n",
      "\u001b[0m14:49:08  17 of 43 START sql table model demo_bronze.syndicated_prospect ................. [RUN]\n",
      "24/01/07 14:49:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:49:10  17 of 43 OK created sql table model demo_bronze.syndicated_prospect ............ [\u001b[32mOK\u001b[0m in 1.78s]\n",
      "\u001b[0m14:49:10  18 of 43 START sql table model demo_silver.daily_market ........................ [RUN]\n",
      "24/01/07 14:49:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:03  18 of 43 OK created sql table model demo_silver.daily_market ................... [\u001b[32mOK\u001b[0m in 112.86s]\n",
      "\u001b[0m14:51:03  19 of 43 START sql table model demo_silver.employees ........................... [RUN]\n",
      "24/01/07 14:51:03 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:05  19 of 43 OK created sql table model demo_silver.employees ...................... [\u001b[32mOK\u001b[0m in 1.99s]\n",
      "\u001b[0m14:51:05  20 of 43 START sql table model demo_silver.date ................................ [RUN]\n",
      "24/01/07 14:51:05 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:07  20 of 43 OK created sql table model demo_silver.date ........................... [\u001b[32mOK\u001b[0m in 1.94s]\n",
      "\u001b[0m14:51:07  21 of 43 START sql table model demo_silver.companies ........................... [RUN]\n",
      "24/01/07 14:51:07 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:10  21 of 43 OK created sql table model demo_silver.companies ...................... [\u001b[32mOK\u001b[0m in 3.17s]\n",
      "\u001b[0m14:51:10  22 of 43 START sql table model demo_silver.accounts ............................ [RUN]\n",
      "24/01/07 14:51:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:14  22 of 43 OK created sql table model demo_silver.accounts ....................... [\u001b[32mOK\u001b[0m in 3.91s]\n",
      "\u001b[0m14:51:14  23 of 43 START sql table model demo_silver.customers ........................... [RUN]\n",
      "24/01/07 14:51:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:18  23 of 43 OK created sql table model demo_silver.customers ...................... [\u001b[32mOK\u001b[0m in 3.35s]\n",
      "\u001b[0m14:51:18  24 of 43 START sql table model demo_silver.trades_history ...................... [RUN]\n",
      "24/01/07 14:51:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:53  24 of 43 OK created sql table model demo_silver.trades_history ................. [\u001b[32mOK\u001b[0m in 35.53s]\n",
      "\u001b[0m14:51:53  25 of 43 START sql table model demo_gold.dim_broker ............................ [RUN]\n",
      "24/01/07 14:51:54 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:56  25 of 43 OK created sql table model demo_gold.dim_broker ....................... [\u001b[32mOK\u001b[0m in 2.18s]\n",
      "\u001b[0m14:51:56  26 of 43 START sql table model demo_gold.dim_date .............................. [RUN]\n",
      "24/01/07 14:51:56 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:57  26 of 43 OK created sql table model demo_gold.dim_date ......................... [\u001b[32mOK\u001b[0m in 1.15s]\n",
      "\u001b[0m14:51:57  27 of 43 START sql table model demo_gold.dim_company ........................... [RUN]\n",
      "24/01/07 14:51:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:51:58  27 of 43 OK created sql table model demo_gold.dim_company ...................... [\u001b[32mOK\u001b[0m in 1.66s]\n",
      "\u001b[0m14:51:58  28 of 43 START sql table model demo_silver.financials .......................... [RUN]\n",
      "24/01/07 14:51:59 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:52:06  28 of 43 OK created sql table model demo_silver.financials ..................... [\u001b[32mOK\u001b[0m in 7.79s]\n",
      "\u001b[0m14:52:06  29 of 43 START sql table model demo_silver.securities .......................... [RUN]\n",
      "24/01/07 14:52:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:52:09  29 of 43 OK created sql table model demo_silver.securities ..................... [\u001b[32mOK\u001b[0m in 2.57s]\n",
      "\u001b[0m14:52:09  30 of 43 START sql table model demo_silver.cash_transactions ................... [RUN]\n",
      "24/01/07 14:52:09 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:52:14  30 of 43 OK created sql table model demo_silver.cash_transactions .............. [\u001b[32mOK\u001b[0m in 5.40s]\n",
      "\u001b[0m14:52:14  31 of 43 START sql table model demo_gold.dim_customer .......................... [RUN]\n",
      "24/01/07 14:52:14 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:52:18  31 of 43 OK created sql table model demo_gold.dim_customer ..................... [\u001b[32mOK\u001b[0m in 4.19s]\n",
      "\u001b[0m14:52:18  32 of 43 START sql table model demo_gold.dim_trade ............................. [RUN]\n",
      "24/01/07 14:52:19 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:52:35  32 of 43 OK created sql table model demo_gold.dim_trade ........................ [\u001b[32mOK\u001b[0m in 16.65s]\n",
      "\u001b[0m14:52:35  33 of 43 START sql table model demo_silver.trades .............................. [RUN]\n",
      "24/01/07 14:52:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:53:02  33 of 43 OK created sql table model demo_silver.trades ......................... [\u001b[32mOK\u001b[0m in 27.02s]\n",
      "\u001b[0m14:53:02  34 of 43 START sql table model demo_gold.dim_security .......................... [RUN]\n",
      "24/01/07 14:53:02 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:53:05  34 of 43 OK created sql table model demo_gold.dim_security ..................... [\u001b[32mOK\u001b[0m in 2.57s]\n",
      "\u001b[0m14:53:05  35 of 43 START sql table model demo_silver.watches_history ..................... [RUN]\n",
      "24/01/07 14:53:05 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:53:15  35 of 43 OK created sql table model demo_silver.watches_history ................ [\u001b[32mOK\u001b[0m in 10.08s]\n",
      "\u001b[0m14:53:15  36 of 43 START sql table model demo_gold.dim_account ........................... [RUN]\n",
      "24/01/07 14:53:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:53:18  36 of 43 OK created sql table model demo_gold.dim_account ...................... [\u001b[32mOK\u001b[0m in 2.70s]\n",
      "\u001b[0m14:53:18  37 of 43 START sql table model demo_silver.holdings_history .................... [RUN]\n",
      "24/01/07 14:53:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:53:27  37 of 43 OK created sql table model demo_silver.holdings_history ............... [\u001b[32mOK\u001b[0m in 9.41s]\n",
      "\u001b[0m14:53:27  38 of 43 START sql table model demo_silver.watches ............................. [RUN]\n",
      "24/01/07 14:53:27 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:53:42  38 of 43 OK created sql table model demo_silver.watches ........................ [\u001b[32mOK\u001b[0m in 15.18s]\n",
      "\u001b[0m14:53:42  39 of 43 START sql table model demo_gold.fact_cash_transactions ................ [RUN]\n",
      "24/01/07 14:53:42 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:53:47  39 of 43 OK created sql table model demo_gold.fact_cash_transactions ........... [\u001b[32mOK\u001b[0m in 4.56s]\n",
      "\u001b[0m14:53:47  40 of 43 START sql table model demo_gold.fact_trade ............................ [RUN]\n",
      "24/01/07 14:53:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:54:01  40 of 43 OK created sql table model demo_gold.fact_trade ....................... [\u001b[32mOK\u001b[0m in 14.22s]\n",
      "\u001b[0m14:54:01  41 of 43 START sql table model demo_gold.fact_holdings ......................... [RUN]\n",
      "24/01/07 14:54:01 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:54:23  41 of 43 OK created sql table model demo_gold.fact_holdings .................... [\u001b[32mOK\u001b[0m in 21.81s]\n",
      "\u001b[0m14:54:23  42 of 43 START sql table model demo_gold.fact_watches .......................... [RUN]\n",
      "24/01/07 14:54:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:54:29  42 of 43 OK created sql table model demo_gold.fact_watches ..................... [\u001b[32mOK\u001b[0m in 5.73s]\n",
      "\u001b[0m14:54:29  43 of 43 START sql table model demo_gold.fact_cash_balances .................... [RUN]\n",
      "24/01/07 14:54:29 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:54:39  43 of 43 OK created sql table model demo_gold.fact_cash_balances ............... [\u001b[32mOK\u001b[0m in 10.80s]\n",
      "\u001b[0m14:54:39  \n",
      "\u001b[0m14:54:39  Finished running 43 table models in 0 hours 7 minutes and 30.89 seconds (450.89s).\n",
      "\u001b[0m14:54:40  \n",
      "\u001b[0m14:54:40  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m14:54:40  \n",
      "\u001b[0m14:54:40  Done. PASS=43 WARN=0 ERROR=0 SKIP=0 TOTAL=43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae93304-68eb-46c8-86a3-26feec06a54a",
   "metadata": {},
   "source": [
    "## Run dbt tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6b563f4-0b31-40a3-b353-9e6c9de8c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m14:55:23  Running with dbt=1.7.3\n",
      "\u001b[0m14:55:24  Registered adapter: spark=1.7.1\n",
      "\u001b[0m14:55:24  Unable to do partial parsing because profile has changed\n",
      "\u001b[0m14:55:28  Found 44 models, 1 test, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m14:55:28  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-34faa2c6-b0af-4967-9d5a-99d660de8164;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 657ms :: artifacts dl 47ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-34faa2c6-b0af-4967-9d5a-99d660de8164\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/15ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 14:55:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 14:55:38 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/07 14:55:38 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/07 14:55:47 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:55:47 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:55:47 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/07 14:55:47 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:55:47 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/01/07 14:56:13 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m14:56:17  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m14:56:17  \n",
      "\u001b[0m14:56:17  1 of 1 START test fact_trade__unique_trade ..................................... [RUN]\n",
      "24/01/07 14:56:19 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m14:56:29  1 of 1 PASS fact_trade__unique_trade ........................................... [\u001b[32mPASS\u001b[0m in 11.66s]\n",
      "\u001b[0m14:56:29  \n",
      "\u001b[0m14:56:29  Finished running 1 test in 0 hours 1 minutes and 0.84 seconds (60.84s).\n",
      "\u001b[0m14:56:29  \n",
      "\u001b[0m14:56:29  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m14:56:29  \n",
      "\u001b[0m14:56:29  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6ce00fe-ce46-4202-a798-a7f227ff3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6a42849c-3708-4890-9653-9efd1850a8b3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 613ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6a42849c-3708-4890-9653-9efd1850a8b3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/33ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 15:00:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 15:00:49 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/07 15:00:50 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/07 15:00:59 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/07 15:00:59 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/07 15:00:59 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/07 15:00:59 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/07 15:00:59 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TBD-TPC-DI-setup\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c892605-9496-4526-b574-a19168678934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/07 15:01:24 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|     bronze|\n",
      "|    default|\n",
      "|demo_bronze|\n",
      "|  demo_gold|\n",
      "|demo_silver|\n",
      "|      digen|\n",
      "|       gold|\n",
      "|     silver|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b33ed00-bdbc-41f2-b7c1-4b620a57eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|database_name|number of tables|\n",
      "+-------------+----------------+\n",
      "|       bronze|               0|\n",
      "|      default|               0|\n",
      "|  demo_bronze|              17|\n",
      "|    demo_gold|              12|\n",
      "|  demo_silver|              14|\n",
      "|        digen|              17|\n",
      "|         gold|               0|\n",
      "|       silver|               0|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "databases = spark.sql(\"show databases\").rdd.map(lambda x : x[0]).collect()\n",
    "number_of_tables = []\n",
    "for database in databases:\n",
    "    spark.sql(f\"use {database}\")\n",
    "    tables = spark.sql(\"show tables\")\n",
    "    number_of_tables.append((database, tables.count()))\n",
    "\n",
    "statistics =  spark.createDataFrame(number_of_tables,[\"database_name\",\"number of tables\"])\n",
    "statistics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a4423ac-c4e4-4daa-bb02-21fb61d1294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"gs://tbd-2023z-1000-data/tpc-di\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e34fa44-084c-4445-ad41-17b4762a0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+--------------------+\n",
      "|_c0|                _c1|       _c2|                 _c3|\n",
      "+---+-------------------+----------+--------------------+\n",
      "|  3|2012-07-11 08:09:52| -37215.14|TGDRsaHPherhApDuH...|\n",
      "| 52|2012-07-07 17:08:29|  -3178.67|PGwhaPC igAVOmHLJ...|\n",
      "| 55|2012-07-12 17:34:13|  -3172.19|uQOUlrpDGHQpeeBGx...|\n",
      "| 61|2012-09-20 03:07:49| -16621.00|VRIGhrJYHmbmNyXtI...|\n",
      "| 28|2012-07-09 07:26:05|  -1315.70|gySbOpZLevgVdfrrw...|\n",
      "| 42|2012-07-08 20:25:37|  -2143.96|iSeQBULkNeQKlgVQy...|\n",
      "|  4|2012-07-21 18:31:28|  -5030.52|TGMAeSwUgWHRucqNG...|\n",
      "| 69|2012-07-08 23:27:28|  -6363.03|QYRSMDtNdFNKIBQJo...|\n",
      "| 42|2012-07-11 03:15:16|  -7995.73|EHHhXQqbPAy QJPju...|\n",
      "| 39|2012-10-04 05:30:27|-309881.64|SVSDTAKjqQOutnTwY...|\n",
      "| 48|2012-08-18 22:19:33|  -1794.97|ZFbSBj APDjoTnlGU...|\n",
      "|  9|2012-08-05 04:17:17| -54226.75|WyvYJTn NaCUMK Kp...|\n",
      "| 84|2012-07-08 16:32:31|  -5173.00|jjLCYCleCIyxDTARU...|\n",
      "| 37|2012-07-09 00:16:49|  -7124.47|XLMjObITsfSkQutEP...|\n",
      "| 13|2012-07-11 18:28:07|-754193.77|sulHeLADnxrgynWJw...|\n",
      "| 15|2012-08-30 23:18:29|  -2942.98|LwFJOyIQuKcQPbLOS...|\n",
      "| 53|2012-09-19 16:03:49|-967891.12|SCEAHvkzoUikCSdDH...|\n",
      "| 55|2012-09-01 05:13:16|  -8910.92|yhhpHXvUACaVJstRT...|\n",
      "| 64|2012-08-11 11:51:40|  -1492.81|nUQEhMemPiLSSdPVw...|\n",
      "| 60|2012-09-08 07:54:43| -94539.83|    DcDMbFbsQmWyNUuy|\n",
      "+---+-------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"delimiter\",\"|\").format(\"csv\").load(f\"{base_path}/CashTransaction.txt\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
